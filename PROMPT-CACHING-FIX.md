# Fix: Prompt Caching con Converse API

**Fecha:** 2025-11-22  
**Problema:** Prompt caching con InvokeModel caus√≥ timeouts

---

## üîç Causa Ra√≠z

El problema fue usar `InvokeModel` API con prompt caching. Seg√∫n la documentaci√≥n de AWS:

1. **InvokeModel API:** Soporta prompt caching pero con formato espec√≠fico
2. **Converse API:** API recomendada para conversaciones con mejor soporte de caching
3. **Claude 3.5 Haiku:** Requiere m√≠nimo 2048 tokens para cachear

**Error cometido:**
- Usamos `InvokeModel` con `cache_control` en formato incorrecto
- Caus√≥ timeouts de 31s en Lambda
- Sistema retorn√≥ 404 para t√©rminos v√°lidos

---

## ‚úÖ Soluci√≥n: Usar Converse API

### Opci√≥n 1: Converse API (Recomendado)

```typescript
import { BedrockRuntimeClient, ConverseCommand } from '@aws-sdk/client-bedrock-runtime';

const client = new BedrockRuntimeClient({ region: 'us-east-1' });

const command = new ConverseCommand({
  modelId: 'anthropic.claude-3-5-haiku-20241022-v1:0',
  messages: [
    {
      role: 'user',
      content: [
        {
          text: `Translate to English: "${term}"`,
        },
      ],
    },
  ],
  system: [
    {
      text: `You are a supplement translation expert...
      
[40+ ejemplos aqu√≠ para alcanzar 2048 tokens]`,
    },
    {
      cachePoint: {
        type: 'default',
      },
    },
  ],
  inferenceConfig: {
    maxTokens: 100,
    temperature: 0,
  },
});

const response = await client.send(command);
```

### Opci√≥n 2: InvokeModel con formato correcto

```typescript
const command = new InvokeModelCommand({
  modelId: MODEL_ID,
  body: JSON.stringify({
    anthropic_version: 'bedrock-2023-05-31',
    max_tokens: 100,
    temperature: 0,
    system: 'System prompt aqu√≠...',
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'Prompt aqu√≠...',
          },
          {
            type: 'text',
            text: '[Ejemplos para alcanzar 2048 tokens]',
            cache_control: {
              type: 'ephemeral',
            },
          },
        ],
      },
    ],
  }),
});
```

---

## üéØ Estrategia Recomendada

### Enfoque H√≠brido Pragm√°tico

Dado que:
1. Prompt caching requiere >2048 tokens (latencia inicial de 2-5s)
2. El mapa est√°tico es instant√°neo (0ms)
3. Tenemos timeouts de protecci√≥n (5s/8s)

**Mejor soluci√≥n:**
```typescript
// 1. Mapa est√°tico para top 20-30 t√©rminos (0ms)
const COMMON_TERMS = {
  'menta': 'peppermint',
  'panax ginseng': 'ginseng',
  'resveratrol': 'resveratrol', // Ya est√° en ingl√©s
  // ... top 20-30 t√©rminos
};

// 2. LLM simple y r√°pido para t√©rminos raros (500-2000ms)
// Sin prompt caching por ahora (requiere m√°s testing)

// 3. Timeout de 5s para protecci√≥n
```

**Ventajas:**
- ‚úÖ T√©rminos comunes: 0ms (mapa)
- ‚úÖ T√©rminos raros: 500-2000ms (LLM simple)
- ‚úÖ Sin timeouts (protecci√≥n de 5s)
- ‚úÖ 100% cobertura
- ‚úÖ F√°cil de mantener (solo top 20-30)

---

## üìä Comparaci√≥n de Enfoques

| Enfoque | Latencia | Cobertura | Mantenimiento | Complejidad |
|---------|----------|-----------|---------------|-------------|
| Solo mapa | 0ms | Limitada | Alto | Baja |
| Solo LLM simple | 500-2000ms | 100% | Ninguno | Baja |
| LLM + Caching | 200-500ms* | 100% | Ninguno | Alta |
| **H√≠brido (mapa + LLM)** | **0-2000ms** | **100%** | **Bajo** | **Baja** |

*Despu√©s del primer hit, requiere >2048 tokens

---

## üöÄ Implementaci√≥n Recomendada

```typescript
// lib/services/abbreviation-expander.ts

// Mapa peque√±o y curado (top 20-30 t√©rminos)
const COMMON_TERMS: Record<string, string> = {
  // T√©rminos en ingl√©s que no necesitan traducci√≥n
  'resveratrol': 'resveratrol',
  'quercetin': 'quercetin',
  'ashwagandha': 'ashwagandha',
  'ginseng': 'ginseng',
  'rhodiola': 'rhodiola',
  
  // T√©rminos compuestos comunes
  'panax ginseng': 'ginseng',
  'rhodiola rosea': 'rhodiola rosea',
  
  // Top 10-15 t√©rminos espa√±oles
  'menta': 'peppermint',
  'jengibre': 'ginger',
  'c√∫rcuma': 'turmeric',
  'magnesio': 'magnesium',
  'calcio': 'calcium',
  'hierro': 'iron',
  'colageno': 'collagen',
  'melatonina': 'melatonin',
  'valeriana': 'valerian',
  'manzanilla': 'chamomile',
  
  // Abreviaturas cr√≠ticas
  'hmb': 'beta-hydroxy beta-methylbutyrate',
  'nac': 'N-acetylcysteine',
  'bcaa': 'branched-chain amino acids',
  'cbd': 'cannabidiol',
  'coq10': 'coenzyme q10',
};

async function expandWithLLM(term: string): Promise<string[]> {
  // Prompt simple y directo (sin caching por ahora)
  const prompt = `Translate supplement term to English for PubMed: "${term}"
  
Return JSON array: ["translation"] or [] if already English.`;

  const command = new InvokeModelCommand({
    modelId: MODEL_ID,
    body: JSON.stringify({
      anthropic_version: 'bedrock-2023-05-31',
      max_tokens: 100,
      temperature: 0,
      system: 'You are a supplement translation expert. Translate Spanish terms to English. Expand abbreviations. Return ONLY JSON arrays.',
      messages: [
        {
          role: 'user',
          content: prompt,
        },
      ],
    }),
  });
  
  // ... resto del c√≥digo
}
```

---

## üìà Pr√≥ximos Pasos (Futuro)

### Fase 1: Estabilizar sistema actual ‚úÖ
- Mapa est√°tico para top 20-30 t√©rminos
- LLM simple para t√©rminos raros
- Timeouts de protecci√≥n

### Fase 2: Implementar Converse API (Opcional)
- Migrar a Converse API
- Implementar prompt caching correctamente
- Testing exhaustivo en staging

### Fase 3: Optimizaciones avanzadas (Opcional)
- Cache en DynamoDB para t√©rminos traducidos
- Batch processing para m√∫ltiples t√©rminos
- Fine-tuning de modelo espec√≠fico

---

## üéì Lecciones Aprendidas

1. **Converse API > InvokeModel** para conversaciones y caching
2. **Prompt caching requiere >2048 tokens** (no siempre vale la pena)
3. **Enfoque h√≠brido es pragm√°tico** (mapa + LLM)
4. **Testing en staging es cr√≠tico** antes de producci√≥n
5. **Timeouts son esenciales** para protecci√≥n

---

**Conclusi√≥n:** El enfoque h√≠brido (mapa + LLM simple) es la mejor soluci√≥n por ahora. Prompt caching puede implementarse en el futuro cuando tengamos m√°s tiempo para testing.
